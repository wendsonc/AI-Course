# -*- coding: utf-8 -*-
"""Questão 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1msYXI3cHQEvpV2I8ta9VSN2VT0pZBAvs

**IMPORTAÇÃO DAS BIBLIOTECAS**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import davies_bouldin_score
import plotly.express as px
import math
from numpy import sqrt

def calculo_distancia(wcss):
    x1, y1 = 2, wcss[0]
    x2, y2 = 20, wcss[len(wcss)-1]

    distances = []
    for i in range(len(wcss)):
        x0 = i+2
        y0 = wcss[i]
        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)
        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)
        distances.append(numerator/denominator)
    
    return distances.index(max(distances)) + 2

data = pd.read_csv('Mall_Customers.csv', delimiter=',')

"""**PRÉ-PROCESSAMENTO**

Para o pré-processamento dos dados foi realizado apenas a exclusão do atributo ID e a transformação de variável categorica em númerica.
"""

new_data = data.drop(['CustomerID'], axis=1) #Excluir os atributos

labelencoder = LabelEncoder()
new_data.iloc[:, 0] = labelencoder.fit_transform(new_data.iloc[:, 0])

print(new_data)

"""**MÉTODO DO COTOVELO (Elbow Method)**

A ideia principal desse método é encontrar um excelente número de clusters para que seja possível formar agrupamentos bem homogeneos. Para isso, foi criado um loop que define um intervalo de 1 a 10 para definir o n_clusters. Todos esses valores são testados buscando o agrupamento que possua a menor soma dos quadrados intra-clusters. O melhor número de clusters, isto é, ponto que indica o equilíbrio entre maior homogeneidade dentro do cluster, vai ser definido pelo ponto da curva mais distante da reta traçada. Esse cálculo vai ser feito pela função "calculo_distancia". A partir disso, os valores de K foram escolhidos.
"""

wcss = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10)
    kmeans.fit(new_data)
    wcss.append(kmeans.inertia_)

n = calculo_distancia(wcss)

print("Número de k:")
print(n)

plt.plot(range(1, 10), wcss)
plt.title("Método do Cotovelo")
plt.xlabel("Número de Clusters")
plt.ylabel("Within-Cluster-Sum-of-Squares")
plt.show()

"""###**Algoritmo KMeans**

Nesse momento, utilizou-se o KMeans do sklearn.cluster para realizar o agrupamento. O método de inicialização foi o K-means++ por oferecer uma rápida convergência e fixando o número máximo de iterações em 300. Abaixo é exibido uma tabela classificando as amostras e, em seguida, o gráfico de dispersão. O processo se repete para valores de K igual 7 e 8.

**Usando K = 4**
"""

kmeans_4 = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 300)
clusters_4 = kmeans_4.fit_predict(new_data) #Retorno os conjuntos que cada variavel vai pertencer

new_data['Agrupamento'] = clusters_4

sb.pairplot(new_data, hue = "Agrupamento")

print(new_data)

"""**Usando K = 8**"""

kmeans_8 = KMeans(n_clusters = 8, init = 'k-means++', max_iter = 300)
clusters_8 = kmeans_8.fit_predict(new_data) #Retorno os conjuntos que cada variavel vai pertencer

new_data['Agrupamento'] = clusters_8

sb.pairplot(new_data, hue = "Agrupamento")

print(new_data)

"""**Usando K = 7**"""

kmeans_7 = KMeans(n_clusters = 7, init = 'k-means++', max_iter = 300)
clusters_7 = kmeans_7.fit_predict(new_data) #Retorno os conjuntos que cada variavel vai pertencer

new_data['Agrupamento'] = clusters_7

print(new_data)

sb.pairplot(new_data, hue = "Agrupamento")

"""###**Clusterização Hierárquico**

O algoritmo de clusterização hierárquico constrói uma hierarquia de clusters, mesclando esses grupos finalizando quando há apenas um único cluster. O número de 'p' define quantas ramificações irão existir até chegar na base. Para escolher o número de clusters, observa-se a maior distância de linhas horizontais que não atravessem um cluster. Observa-se que o valor é igual ao K que foi escolhido para o algoritmo anterior. Assim, serão utilizados os mesmos valores de K (4, 7 e 8).
"""

x = new_data.to_numpy()
dendrogram(linkage(x, 'ward'), truncate_mode='level', p=5)
plt.show()

"""**Com linkage = ward e K = 4, 7 e 8**

**Usando K = 4**
"""

new_data_2 = np.array(new_data.drop(['Agrupamento'], axis=1))
array_cluster = pd.DataFrame(new_data_2, columns=['Genre','Age','Annual Income (k$)','Spending Score (1-100)']) #Adicionar rotulos
modelo = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters = 4)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()

sb.pairplot(array_cluster, hue = "Cluster_HC")

"""**Usando K = 7**"""

modelo = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters = 7)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()


sb.pairplot(array_cluster, hue = "Cluster_HC")

"""**Usando K = 8**"""

modelo = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters = 8)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()

sb.pairplot(array_cluster, hue = "Cluster_HC")

"""**Com linkage = average e K = 4, 7 e 8**

**Usando K = 4**
"""

modelo = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters = 4)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()

sb.pairplot(array_cluster, hue = "Cluster_HC")

"""**Usando K = 8**"""

modelo = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters = 8)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()

sb.pairplot(array_cluster, hue = "Cluster_HC")

"""**Usando K = 7** """

modelo = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters = 7)
grupos = modelo.fit_predict(array_cluster)

array_cluster['Cluster_HC'] = grupos
array_cluster.head()

sb.pairplot(array_cluster, hue = "Cluster_HC")

"""###**Análise dos algoritmos de agrupamento**

Para realizar uma breve análise do desempenho desses algoritmos, foi realizado um cálculo de pontuação Davies-Bouldin, um indice interno de medida de qualidade baseado em métricas de distância inter-cluster. Esse cálculo dar-se pela medida de similaridade média de cada cluster com seu cluster mais semelhante, em que valores próximos de zero representam bons resultados. Nas duas funções abaixo foi utilizado o melhor número de cluster, K = 7 (escolhido baseado no método do cotovelo). Fica evidênciado que a clusterização hierárquica mostrou resultados melhores.

**Algoritmo KMeans**
"""

labels = kmeans_7.labels_
score_k = davies_bouldin_score(new_data, labels)
print(score_k)

"""**Clusterização Hierárquico**"""

labels = modelo.labels_
score_hc = davies_bouldin_score(array_cluster, labels)
print(score_hc)